{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iotpelican/ml-llm-course-work/blob/main/ss_T27_hw_03_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHLcriKWLRe4"
      },
      "source": [
        "# Assignment 03 - Linear Regression and Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "uphXzmnaFyDy"
      },
      "source": [
        "### <span style=\"color:maroon\"> Assignment Submission Instructions </span>\n",
        "\n",
        "Please note that your homework won’t be graded if your notebook doesn’t include the output. Before submitting to Gradescope, make sure to run all the cells so that the output is visible. If you're using Google Colab: Go to Edit > Notebook settings and uncheck the box that says “Omit code cell output when saving,” otherwise your output won’t be saved.Points may also be taken off if these guidelines aren’t followed.\n",
        "    \n",
        "Make sure to comment your code so others can easily understand what it does. Each graph should include a title, axis labels, and a legend if necessary. The goal is for each graph to be clear and understandable on its own. Try to avoid using the global namespace too much—it's best to keep your code organized inside functions whenever possible. When you're done, upload your .ipynb file to Canvas.\n",
        "\n",
        "This assignment introduces you to linear regression using both scikit-learn and a manual implementation of gradient descent. You will explore a real-world dataset (California Housing Dataset). You will build an initial linear regression model using scikit-learn and implement gradient descent step-by-step. You will then compare learning behavior with different hyperparameters and interpret and evaluate model performance\n",
        "\n",
        "\n",
        "Submit .ipynb file with all code and outputs.\n",
        "\n",
        "Comment your code where asked.\n",
        "\n",
        "Answer all questions clearly in markdown cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nNOD-Z7SzAq"
      },
      "source": [
        "## Part 0: Load and Explore Dataset (20 points)\n",
        "The code provided below performs a list of operations such as loading the dataset, extracting MedInc as feature and MedHouseVal as target, splitting into train/test sets and displaying first 5 rows along with the data shape. Run the cell once to check it is displaying the appropriate ifnormation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWlmuAMwTZ3P"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "california = fetch_california_housing(as_frame=True)\n",
        "df = california.frame\n",
        "\n",
        "# Select feature and target\n",
        "data = df[['MedInc', 'MedHouseVal']]\n",
        "\n",
        "# Display first 5 rows and shape\n",
        "print(data.head())\n",
        "print(data.shape)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data[['MedInc']]\n",
        "y = data['MedHouseVal']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display shapes of train/test splits\n",
        "#print(\"Train shape:\", X_train.shape, y_train.shape)\n",
        "#print(\"Test shape:\", X_test.shape, y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cfNlkCNFyDz"
      },
      "source": [
        "\n",
        "Questions-\n",
        "\n",
        "1. What relationship do you expect between Median Income and Median House Value?\n",
        "\n",
        "2. Why might Median Income be a good predictor for house prices?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4wvvzKoUIAN"
      },
      "source": [
        "---\n",
        "### Part 1: Linear Regression with Scikit-learn (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWJqQxhHFyD0"
      },
      "source": [
        "Task:\n",
        "\n",
        "1. Fit LinearRegression model on training data\n",
        "\n",
        "2. Print learned intercept and coefficient\n",
        "\n",
        "3. Plot training data and regression line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJtwrjdO6TbS"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "reg = LinearRegression()\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Intercept: {reg.intercept_}\")\n",
        "print(f\"Coefficient: {reg.coef_[0]}\")\n",
        "\n",
        "plt.scatter(X_train, y_train, color='blue', alpha=0.5, label='Training data')\n",
        "plt.plot(X_train, reg.predict(X_train), color='red', label='Regression line')\n",
        "plt.xlabel(\"Median Income (MedInc)\")\n",
        "plt.ylabel(\"Median House Value (MedHouseVal)\")\n",
        "plt.title(\"Linear Regression with scikit-learn\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWQrNaSVFyD0"
      },
      "source": [
        "\n",
        "Questions-\n",
        "\n",
        "\n",
        "1. What does the slope (coefficient) tell you about the relationship between Median Income and House Value?\n",
        "\n",
        "2. What does the intercept represent in this context?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8DfbwAXFyD0"
      },
      "source": [
        "---\n",
        "### Part 2: Feature Scaling and Gradient Descent Implementation (20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkYvWokgFyD0"
      },
      "source": [
        "Task:\n",
        "\n",
        "1. Scale features and target with StandardScaler\n",
        "\n",
        "2. Implement gradient descent on scaled data for 1000 iterations, learning rate 0.01\n",
        "\n",
        "3. Print loss every 100 iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIl3TjZgFyD0"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Scale features and target\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1,1)).flatten()\n",
        "\n",
        "X_train_np = X_train_scaled.flatten()\n",
        "y_train_np = y_train_scaled\n",
        "n = len(X_train_np)\n",
        "\n",
        "w = 0.0\n",
        "b = 0.0\n",
        "lr = 0.01\n",
        "num_iterations = 1000\n",
        "losses = []\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    y_pred = w * X_train_np + b\n",
        "    error = y_pred - y_train_np\n",
        "\n",
        "    loss = np.mean(error ** 2)\n",
        "    losses.append(loss)\n",
        "\n",
        "    # Calculate gradients (dw, db)\n",
        "    # YOUR CODE HERE\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # Update weights\n",
        "    # YOUR CODE HERE\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Iteration {i}, Loss: {loss:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X47el0TFyD0"
      },
      "outputs": [],
      "source": [
        "#Plot Loss Curve\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#plt.plot(losses)\n",
        "#plt.xlabel(\"Iteration\")\n",
        "#plt.ylabel(\"Mean Squared Error Loss\")\n",
        "#plt.title(\"Loss Curve - Gradient Descent\")\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjRUxdI-FyD0"
      },
      "source": [
        "\n",
        "Questions-\n",
        "\n",
        "1. Does the loss decrease consistently during gradient descent training?\n",
        "\n",
        "2. What might happen if the learning rate was too high or too low?\n",
        "\n",
        "3. Why is feature scaling important for gradient descent to work effectively here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKx-jvivFyD0"
      },
      "source": [
        "---\n",
        "### Part 3: Experimenting with Learning Rates (20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JnK4wGmFyD0"
      },
      "source": [
        "Task:\n",
        "\n",
        "1. Repeat gradient descent with learning rates 0.0001, 0.01, and 0.5\n",
        "\n",
        "2. Plot loss curves on same graph\n",
        "\n",
        "3. Observe and describe convergence behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRNJ-BN9FyD0"
      },
      "outputs": [],
      "source": [
        "learning_rates = [0.0001, 0.01, 0.5]\n",
        "loss_curves = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    w_temp, b_temp = 0.0, 0.0\n",
        "    losses_temp = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        y_pred_temp = w_temp * X_train_np + b_temp\n",
        "        error_temp = y_pred_temp - y_train_np\n",
        "\n",
        "        loss_temp = np.mean(error_temp ** 2)\n",
        "        losses_temp.append(loss_temp)\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    loss_curves[lr] = losses_temp\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "for lr in learning_rates:\n",
        "    plt.plot(loss_curves[lr], label=f\"lr={lr}\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Mean Squared Error Loss\")\n",
        "plt.title(\"Loss Curves for Different Learning Rates\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrRcMOLrFyD0"
      },
      "source": [
        "\n",
        "Questions-\n",
        "\n",
        "1. Which learning rate showed the best convergence behavior?\n",
        "\n",
        "2. Did any of the runs diverge or converge too slowly? Why?\n",
        "\n",
        "3. What happens if the learning rate is too large or too small in gradient descent?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rTxbD9aFyD0"
      },
      "source": [
        "---\n",
        "### Part 4: Evaluating Gradient Descent (20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zND1NnyaFyD0"
      },
      "source": [
        "Task:\n",
        "\n",
        "1. Predict on scaled test data using final w, b from Part 2\n",
        "\n",
        "2. Inverse-transform predictions to original scale\n",
        "\n",
        "3. Calculate and print MAE, MSE, RMSE\n",
        "\n",
        "4. Compare with scikit-learn model predictions on original test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIkVqyXfFyD1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Predict on scaled test data\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1,1)).flatten()\n",
        "\n",
        "y_pred_scaled = w * X_test_scaled.flatten() + b\n",
        "\n",
        "# Inverse transform predictions\n",
        "y_pred_gd = scaler_y.inverse_transform(y_pred_scaled.reshape(-1,1)).flatten()\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred_gd)\n",
        "mse = mean_squared_error(y_test, y_pred_gd)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"Gradient Descent Model:\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "\n",
        "# scikit-learn predictions\n",
        "y_pred_skl = reg.predict(X_test)\n",
        "mae_skl = mean_absolute_error(y_test, y_pred_skl)\n",
        "mse_skl = mean_squared_error(y_test, y_pred_skl)\n",
        "rmse_skl = np.sqrt(mse_skl)\n",
        "\n",
        "print(f\"\\nscikit-learn Model:\")\n",
        "print(f\"MAE: {mae_skl:.4f}\")\n",
        "print(f\"MSE: {mse_skl:.4f}\")\n",
        "print(f\"RMSE: {rmse_skl:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjgvrV7OFyD1"
      },
      "source": [
        "\n",
        "Questions-\n",
        "\n",
        "1. How do the evaluation metrics from your gradient descent model compare with scikit-learn's?\n",
        "\n",
        "2. Which model performs better and why do you think that is?\n",
        "\n",
        "3. What could be reasons for differences in performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0Zpx79_aQEC"
      },
      "source": [
        "*Your answer:*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "copyright",
        "xxOhpvdW6TbX",
        "exercise-1-key-1",
        "43ZTSJEc526U",
        "exercise-5-key-1",
        "ubHispCAA_5u",
        "exercise-6-key-1",
        "5p1IvWjfEjqm",
        "exercise-9-key-1"
      ],
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}